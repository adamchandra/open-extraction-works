* Textworks as a service

** Service Subsystems

*** REST Portal
    - listen for CSV inputs
    - respond via REST with results and status

*** Upload Ingestor
    - filter out already spidered/extracted URLs
    - write out spidering-urls.csv
      - (or communicate to spider via redis/kafka/etc)

*** Spider/Scraper
    - Targeted scraping of explicit URLs
    - Spidering done with either Scrapy, browser (via selenium), or axios
    - Uses ELK to filter its scraping (to avoid re-spidering 301/2 before getting to cached files)
    - log output to ELK to register completed spidering files

*** HTML field extractor
    Current recognized fields: Abstracts (next up: PDF urls)

*** Corpus Management
    Hierarchical directory structuring with hashed-prefix paths

*** Q/A Editing
    Review extraction choices and interactively approve or mark errors
