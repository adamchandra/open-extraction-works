* Textworks as a service
   
** Service Subsystems
   
*** REST Portal 
    - listen for CSV inputs
    - filter out already spidered/extracted URLs
    - write out spidering-urls.csv
      - (or communicate to spider via redis/kafka/etc)
    - respond via REST with results and status
      
*** Elasticsearch
    - Elasticsearch is single-source-of-truth for overall state of processing
    - All logs pushed by filebeat from other subsystems to ES.

*** Spider/Scraper
    - Targeted scraping of explicit URLs
    - Spidering done with either Scrapy, browser (via selenium), or axios
    - Uses ELK to filter its scraping (to avoid re-spidering 301/2 before getting to cached files)
    - log output to ELK to register completed spidering files
    
*** HTML field extractor
    Current recognized fields: Abstracts (next up: PDF urls)

*** Corpus Management
    Hierarchical directory structuring with hashed-prefix paths
    
*** Q/A Editing
    Review extraction choices and interactively approve or mark errors 
    

** Pipeline Topology
   
** Data Structure
   


